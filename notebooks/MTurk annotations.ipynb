{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTurk annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_results_file(dir_file,city):\n",
    "    #Defining labels\n",
    "    labels=['Dangerous', 'Dirty', 'Pretty', 'Preserved', 'Accessible', 'Interesting','Picturesque', 'Wealthy', 'Quiet', 'Polluted', 'Pleasant', 'Happy']\n",
    "    #Reading original file\n",
    "    df_mturk=pd.read_csv(dir_file,delim_whitespace=True)\n",
    "    #Renaming columns: Answer.Ax with actual label and the annotation to annotation_url\n",
    "    columns_to_rename={\"Answer.A\"+str(i+1):x for i,x in enumerate(labels)}\n",
    "    columns_to_rename.update({\"annotation\":\"annotation_url\"})\n",
    "    df_mturk.rename(columns=columns_to_rename,inplace=True)\n",
    "    #Getting the image name\n",
    "    df_mturk[\"annotation\"]=df_mturk[\"annotation_url\"].str.split(\"/\",expand=True)[6]\n",
    "    #Returning the ordered dataframe\n",
    "    df_mturk[\"city\"]=city\n",
    "    return df_mturk[[\"annotation\"]+labels+[\"city\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_mturk_gc=\"/Users/lemr/Documents/EPFL/Tercer_Semestre/COM-405_Optional_project_in_communication_systems/datasets/MTurk_annotations/mturk-results/hit-gc.results\"\n",
    "dir_mturk_sc=\"/Users/lemr/Documents/EPFL/Tercer_Semestre/COM-405_Optional_project_in_communication_systems/datasets/MTurk_annotations/mturk-results/hit-sc.results\"\n",
    "dir_mturk_lc=\"/Users/lemr/Documents/EPFL/Tercer_Semestre/COM-405_Optional_project_in_communication_systems/datasets/MTurk_annotations/mturk-results/hit-lc.results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mturk_gc=read_results_file(dir_mturk_gc,\"gc\")\n",
    "df_mturk_sc=read_results_file(dir_mturk_sc,\"sc\")\n",
    "df_mturk_lc=read_results_file(dir_mturk_lc,\"lc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting individual annotations by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_DataFrame_label(df,label,image_column,num_raters):\n",
    "    #Grouping by the images\n",
    "    df_label=df.groupby(image_column,as_index=True).apply(lambda x: list(x[label])).reset_index()\n",
    "    df_label.rename(columns={0:\"list_rates\"},inplace=True)\n",
    "    #Getting rates\n",
    "    rates = [df_label, pd.DataFrame(df_label[\"list_rates\"].tolist()).iloc[:, :num_raters]]\n",
    "    #Getting each rating in a different column\n",
    "    df_label=pd.concat(rates, axis=1).drop([\"list_rates\"], axis=1)\n",
    "    #Renaming the columns\n",
    "    df_label.rename(columns={i:\"rater_\"+str(i+1) for i in range(0,num_raters)},inplace=True)\n",
    "    df_label.to_csv(\"datasets/MTurk_annotations/mturk_gto_hits_\"+str(label)+\".csv\",index=False)\n",
    "    print(\"csv file generated in: \"+\"datasets/MTurk_annotations/mturk_gto_hits_\"+str(label)+\".csv\")\n",
    "    df_label[\"median\"]=df_label.drop(image_column,axis=1).median(axis=1)\n",
    "    print(\"\\n\")\n",
    "    print(\"Value counts for median:\")\n",
    "    print(\"mean: \"+str(round(df_label[\"median\"].mean(),2)))\n",
    "    print(\"std: \"+str(round(df_label[\"median\"].std(),2)))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_cities=df_mturk_gc.append([df_mturk_sc,df_mturk_lc],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Dangerous.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 2.98\n",
      "std: 0.99\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Dirty.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.16\n",
      "std: 1.06\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Pretty.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.11\n",
      "std: 1.02\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Preserved.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.84\n",
      "std: 1.05\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Accessible.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 4.69\n",
      "std: 0.94\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Interesting.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.84\n",
      "std: 0.94\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Picturesque.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.09\n",
      "std: 1.06\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Wealthy.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 2.64\n",
      "std: 0.88\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Quiet.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.47\n",
      "std: 1.0\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Polluted.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 2.89\n",
      "std: 0.92\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Pleasant.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.82\n",
      "std: 0.97\n",
      "\n",
      "\n",
      "csv file generated in: datasets/MTurk_annotations/mturk_gto_hits_Happy.csv\n",
      "\n",
      "\n",
      "Value counts for median:\n",
      "mean: 3.67\n",
      "std: 0.95\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dangerous\n",
    "df_dangerous=get_DataFrame_label(df=df_all_cities,label=\"Dangerous\",image_column=\"annotation\",num_raters=10)\n",
    "#Dirty\n",
    "df_dirty=get_DataFrame_label(df=df_all_cities,label=\"Dirty\",image_column=\"annotation\",num_raters=10)\n",
    "#Pretty\n",
    "df_pretty=get_DataFrame_label(df=df_all_cities,label=\"Pretty\",image_column=\"annotation\",num_raters=10)\n",
    "#Preserved\n",
    "df_preserved=get_DataFrame_label(df=df_all_cities,label=\"Preserved\",image_column=\"annotation\",num_raters=10)\n",
    "#Accessible\n",
    "df_accessible=get_DataFrame_label(df=df_all_cities,label=\"Accessible\",image_column=\"annotation\",num_raters=10)\n",
    "#Interesting\n",
    "df_interesting=get_DataFrame_label(df=df_all_cities,label=\"Interesting\",image_column=\"annotation\",num_raters=10)\n",
    "#Picturesque\n",
    "df_picturesque=get_DataFrame_label(df=df_all_cities,label=\"Picturesque\",image_column=\"annotation\",num_raters=10)\n",
    "#Wealthy\n",
    "df_wealthy=get_DataFrame_label(df=df_all_cities,label=\"Wealthy\",image_column=\"annotation\",num_raters=10)\n",
    "#Quiet\n",
    "df_quiet=get_DataFrame_label(df=df_all_cities,label=\"Quiet\",image_column=\"annotation\",num_raters=10)\n",
    "#Polluted\n",
    "df_polluted=get_DataFrame_label(df=df_all_cities,label=\"Polluted\",image_column=\"annotation\",num_raters=10)\n",
    "#Pleasant\n",
    "df_pleasant=get_DataFrame_label(df=df_all_cities,label=\"Pleasant\",image_column=\"annotation\",num_raters=10)\n",
    "#Happy\n",
    "df_happy=get_DataFrame_label(df=df_all_cities,label=\"Happy\",image_column=\"annotation\",num_raters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting random (5) annotations by label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider the labels that were used in the CECYTE's crowdsourcing experiment:\n",
    "* Dangerous\n",
    "* Dirty\n",
    "* Pretty\n",
    "* Interesting\n",
    "* Polluted\n",
    "* Pleasant\n",
    "\n",
    "For this, we will take 5 random annotations for each image and do this 10 times, in order to get an average, so that we can compare CECYTE's results (these only have 5 raters) with Mturk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_raters(df,num_raters,num_annotations,num_samples,image_column,label):\n",
    "    flag=0\n",
    "    for i in range(num_samples):\n",
    "        ########################\n",
    "        #Getting random raters:#\n",
    "        ######################## \n",
    "        raters=[]\n",
    "        count=0\n",
    "        for j in range(num_annotations):\n",
    "            rand_rater=randint(1,num_annotations)\n",
    "            if \"rater_\"+str(rand_rater) not in raters[:]:\n",
    "                raters.append(\"rater_\"+str(rand_rater))\n",
    "                count+=1\n",
    "            if count==num_raters:\n",
    "                flag+=1\n",
    "                break\n",
    "        ##########################################################\n",
    "        #Getting a new csv file for each set of the random raters#\n",
    "        ##########################################################\n",
    "        df[[image_column]+raters].to_csv(\"datasets/MTurk_annotations/\"+str(label)+\"/mturk_gto_hits_\"+str(label)+\"_\"+str(i+1)+\".csv\"\n",
    "                                        ,index=False)\n",
    "    if flag==num_samples: print(\"Number of raters reached correctly\\n\")\n",
    "    print(\"csv files generated at: \"+ \"datasets/MTurk_annotations/\"+str(label)+\"/\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raters reached correctly\n",
      "\n",
      "csv files generated at: datasets/MTurk_annotations/Dangerous/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Dangerous\n",
    "get_random_raters(df_dangerous,5,10,10,\"annotation\",\"Dangerous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raters reached correctly\n",
      "\n",
      "csv files generated at: datasets/MTurk_annotations/Dirty/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Dirty\n",
    "get_random_raters(df_dirty,5,10,10,\"annotation\",\"Dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raters reached correctly\n",
      "\n",
      "csv files generated at: datasets/MTurk_annotations/Pretty/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Pretty\n",
    "get_random_raters(df_pretty,5,10,10,\"annotation\",\"Pretty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raters reached correctly\n",
      "\n",
      "csv files generated at: datasets/MTurk_annotations/Interesting/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Interesting\n",
    "get_random_raters(df_interesting,5,10,10,\"annotation\",\"Interesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raters reached correctly\n",
      "\n",
      "csv files generated at: datasets/MTurk_annotations/Polluted/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Polluted\n",
    "get_random_raters(df_polluted,5,10,10,\"annotation\",\"Polluted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raters reached correctly\n",
      "\n",
      "csv files generated at: datasets/MTurk_annotations/Pleasant/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Pleasant\n",
    "get_random_raters(df_pleasant,5,10,10,\"annotation\",\"Pleasant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a consolidated Dataframe with 5 annotations per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting a consolidated Dataframe with 5 annotations per image.\n",
    "labels_6=[\"Dangerous\",\"Dirty\",\"Pretty\",\"Interesting\",\"Polluted\",\"Pleasant\"]\n",
    "files=[\"datasets/MTurk_annotations/\"+str(label)+\"/mturk_gto_hits_\"+str(label)+\"_1.csv\" for label in labels_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dangerous_5=pd.read_csv(files[0])\n",
    "df_dirty_5=pd.read_csv(files[1])\n",
    "df_pretty_5=pd.read_csv(files[2])\n",
    "df_interesting_5=pd.read_csv(files[3])\n",
    "df_polluted_5=pd.read_csv(files[4])\n",
    "df_pleasant_5=pd.read_csv(files[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dangerous_5[\"median\"]=df_dangerous_5.median(axis=1)\n",
    "df_dirty_5[\"median\"]=df_dirty_5.median(axis=1)\n",
    "df_pretty_5[\"median\"]=df_pretty_5.median(axis=1)\n",
    "df_interesting_5[\"median\"]=df_interesting_5.median(axis=1)\n",
    "df_polluted_5[\"median\"]=df_polluted_5.median(axis=1)\n",
    "df_pleasant_5[\"median\"]=df_pleasant_5.median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mturk_all_labels=pd.DataFrame()\n",
    "df_mturk_all_labels[\"annotation\"]=df_dangerous_5[\"annotation\"].copy()\n",
    "\n",
    "#Dangerous\n",
    "df_mturk_all_labels=pd.merge(df_mturk_all_labels,df_dangerous_5[[\"annotation\",\"median\"]],\n",
    "                              how=\"inner\",on=\"annotation\").rename(columns={\"median\":\"Dangerous\"})\n",
    "#Dirty\n",
    "df_mturk_all_labels=pd.merge(df_mturk_all_labels,df_dirty_5[[\"annotation\",\"median\"]],\n",
    "                              how=\"inner\",on=\"annotation\").rename(columns={\"median\":\"Dirty\"})\n",
    "#Pretty\n",
    "df_mturk_all_labels=pd.merge(df_mturk_all_labels,df_pretty_5[[\"annotation\",\"median\"]],\n",
    "                              how=\"inner\",on=\"annotation\").rename(columns={\"median\":\"Pretty\"})\n",
    "#Interesting\n",
    "df_mturk_all_labels=pd.merge(df_mturk_all_labels,df_interesting_5[[\"annotation\",\"median\"]],\n",
    "                              how=\"inner\",on=\"annotation\").rename(columns={\"median\":\"Interesting\"})\n",
    "#Polluted\n",
    "df_mturk_all_labels=pd.merge(df_mturk_all_labels,df_polluted_5[[\"annotation\",\"median\"]],\n",
    "                              how=\"inner\",on=\"annotation\").rename(columns={\"median\":\"Polluted\"})\n",
    "#Pleasant\n",
    "df_mturk_all_labels=pd.merge(df_mturk_all_labels,df_pleasant_5[[\"annotation\",\"median\"]],\n",
    "                              how=\"inner\",on=\"annotation\").rename(columns={\"median\":\"Pleasant\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mturk_all_labels.to_csv(\"datasets/MTurk_annotations/mturk_6_labels_5_annotators.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
